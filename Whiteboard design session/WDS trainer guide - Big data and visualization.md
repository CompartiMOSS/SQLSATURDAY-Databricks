# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements.

- Current customer infrastructure and architecture.

- Potential issues, objectives and blockers.

**Step 2: Design a proof of concept solution (60 minutes)**

Outcome: Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

Outcome: Present solution to your customer.

- Present solution.

- Respond to customer objections.

- Receive feedback.

**Wrap-up (15 minutes)**

- Review preferred solution.

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Prior to the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

**Have fun**! Encourage participants to have fun and share!

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Big data and visualization whiteboard design session student guide

## Abstract and learning objectives

In this whiteboard design session, you will work with a group to design a solution for ingesting and preparing historic flight delay and weather data, and creating, training, and deploying a machine learning model that can predict flight delays.

At the end of this whiteboard design session you will have learned how to include a web application that obtains weather forecasts from a 3rd party, collects flight information from end users, and sends that information to the deployed machine learning model for scoring. Part of the exercise will include providing visualizations of historic flight delays, and orchestrating the collection and batch scoring of historic and new flight delay data.

## Step 1: Review the customer case study

**Outcome**

Analyze your customer’s needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator or SME presents an overview of the customer case study along with technical tips.

1. Meet your table participants and trainer.
2. Read all of the directions for steps 1–3 in the student guide.
3. As a table team, review the following customer case study.

### Customer situation

Margie's Travel (MT) provides concierge services for business travelers. In an increasingly crowded market, they are always looking for ways to differentiate themselves and provide added value to their corporate customers.

MT is investigating ways that they can capitalize on their existing data assets to provide new insights that provide them a strategic advantage against their competition. In planning their product, they heard much fanfare about machine learning and came up with the idea of using predictive analytics to help customers best select their travels based on the likelihood of a delay. When reviewing their customer transaction histories, they discovered that their most premium customers often book their travel within 7 days of departure. In speaking with customer service, they learned that these customers often ask questions like, "I don't have to be there until Tuesday, so is it better for me to fly out on Sunday or Monday?"

While there are many factors that customer service uses to tailor their guidance to the customer (such as cost and travel duration), MT believes an innovative solution might come in the form of giving the customer an assessment of the risk of encountering flight delays. For low risk flights, the customer may choose to book with a narrower travel window, giving them more precious time at home and less on the road spent arriving too early to a destination. MT is interested in applying data science to the problem to discover if the weather forecast coupled with their historical flight delay data could be used to provide a meaningful input into the customer's decision-making process.

MT plans to pilot this solution internally, whereby the small population of customer support who service MT's premium tier of business travelers would begin using the solution and offering it as an additional data point for travel optimization. They would like to provide their customer support agents a web-based solution that enables them to map the predicted delays for a particular customer's departure airport(s) of choice.

MT has over 30 years of historical flight data provided to them by the United States Department of Transportation (USDOT), which among other data points includes flight delay information for every flight. The data arrives in flat, comma separated value (CSV) files with a schema of the following:

(Year, Month, DayOfMonth, Airline, TailNum, FlightNum, OriginAirport, DestinationAirport, ScheduledDepartureTime, ActualDepartureTime, ScheduledArrivalTime, DepartureDelay, AirTime, Distance, Cancelled, CancellationCode)

In addition, for all data since 2003, each row includes new fields describing the type of delay experienced, where the value for each type is the number of minutes the delay was experienced for that source of delay:

(CarrierDelay, WeatherDelay, NationalAirSystemDelay, SecurityDelay, LateAircraftDelay)

They receive updates to this data monthly, where the flight data and other related files total about 1 GB. In total their solution currently manages about 2 TB worth of data.

Additionally, they receive current and forecasted weather data from a third-party service. This service gives them the ability to receive weather forecasts around any airport, and provides forecasts up to 10 days. They have a history of the historical weather condition for each flight as CSV files, but acquiring the weather forecasts requires a call to a REST API that returns a JSON (JavaScript Object Notation) structure. Each airport of interest needs to be queried individually. An excerpt of the weather forecast for a single day at the Seattle-Tacoma International airport is as follows:

```json
{
  "date": {
    "epoch": "1444701600",
    "pretty": "7:00 PM PDT on October 12, 2015",
    "day": 12,
    "month": 10,
    "year": 2015,
    "yday": 284,
    "hour": 19,
    "min": "00",
    "sec": 0,
    "ampm": "PM",
    "tz_short": "PDT",
    "tz_long": "America/Los_Angeles"
  },
  "high": {
    "fahrenheit": "64",
    "celsius": "18"
  },
  "low": {
    "fahrenheit": "54",
    "celsius": "12"
  },
  "conditions": "Overcast",
  "maxwind": {
    "mph": 15,
    "kph": 24,
    "dir": "SSW",
    "degrees": 209
  },
  "avewind": {
    "mph": 10,
    "kph": 16,
    "dir": "SSW",
    "degrees": 209
  },
  "avehumidity": 70,
  "maxhumidity": 0,
  "minhumidity": 0
}
```

Jack Tradewinds, the CIO of MT, is looking to modernize their data story. He has heard a great deal of positive news about Spark SQL on HDInsight and its ability to query exactly the type of files he has in a performant way, but also in a way that is more familiar to his analysts and developers because they are all familiar with the SQL syntax that it supports. He would love to understand if they can move this data away from their on-premises datacenter into the cloud, and enhance their ability to load, process, and analyze it going forward. Given his long-standing relationship with Microsoft, he would like to see if Azure can meet his needs.

### Customer needs

1. Want to modernize their analytics platform, without sacrificing the ability to query their data using SQL.

2. Need an approach that can store all of their data, including the unmodified source data and the cleansed data from which they query for production purposes.

3. Want to understand how they will load their large quantity of historical data into Azure.

4. Need to be able to query the weather forecast and use it as input to their flight delay predictions.

5. Desire a proof of concept (PoC) machine learning model that takes as input their historical data on flight delays and weather conditions in order to identify whether a flight is likely to be delayed or not.

6. Need web-based visualizations of the flight delay predictions.

### Customer objections

1. We have heard that creating a machine learning model takes a month to build and another 2-3 months to operationalize so that it is useable from our production systems. Is this true?

2. Once our model is operationalized, how do we retrain and redeploy it? Will this process break clients currently accessing the deployed model?

3. Can we query flat files in the file system using SQL?

4. Does Azure provide anything that would speed up querying (and exploration) of files in Hadoop Distributed File Systems (HDFS)?

5. Does Azure provide any tools for visualizing our data? Ideally access to these could be managed with Active Directory.

6. Can we use Azure Active Directory accounts for our users, and if so, can we restrict who can access Azure Databricks, when they can access it, require two-factor authentication, and restrict access if there is suspicious activity on their account?

7. Is Azure Databricks our only option for running SQL on Hadoop solutions in Azure?

8. We have heard of Azure Data Lake, but we are not clear about whether this is currently a good fit for our PoC solution, or whether we should be using it for interactive analysis of our data.

### Infographic for common scenarios

![The Data Analytics diagram is broken into three sections: On-Premises, Azure, and End Users. On-Premises includes a Web Server with log files, and an end user with a computer and a portable device. The Azure section includes three parts: Generation (Azure website and log files), Storage (Azure SQL Database and Blob Storage), and Data Processing (SQL Data Warehouse, Machine Learning, and HDInsight (Hadoop). The End Users section has Business Intelligence, and End Users with portable devices.](media/common-scenarios.png 'Data Analytics diagram')

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With all participants at your table, answer the following questions and list the answers on a flip chart:

1. Who should you present this solution to? Who is your target customer audience? Who are the decision makers?
2. What customer business needs do you need to address with your solution?

**Design**

Directions: With all participants at your table, respond to the following questions on a flip chart:

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for data loading, data preparation, storage, machine learning modeling, and reporting. You will refine this diagram as you proceed.

_Data loading_

1. How would you recommend that MT get their historical data into Azure? What services would you suggest and what are the specific steps they would need to take to prepare the data, to transfer the data, and where would the loaded data land?

2. Update your diagram with the data loading process with the steps you identified.

_Data preparation_

1. What service would you recommend MT capitalize on to explore the flat files they get from the USDOT using SQL?

2. What specific configuration would you use? What components of Azure Databricks would you use to allow MT analysts to query and prep the data? How would they author and execute these data prep tasks?

3. How would you suggest MT integrate weather forecast data?

_Machine learning modeling_

1. What technology would you recommend that MT use for implementing their machine learning model?

2. How would you guide MT to load data, so it can be processed by the machine learning model?

3. What category of machine learning algorithm would you recommend to MT for use in constructing their model? For this scenario your option is clustering, regression or two-class classification. Why?

4. Assuming you selected an algorithm that requires training, address the following model design questions:

   a. What is the high-level flow of your machine learning model?

   b. What attributes of the flight and weather data do you think MT should use in predicting flight delays? How would you recommend that MT identify the columns that provide the most predictive value in determining if a flight will be delayed? Be specific on the particular modules or libraries they could use and how they would apply them against the data.

   c. Some of the data may need a little touching up: columns need to be removed, data types need to be changed. How would these steps be applied in your model?

   d. How would you recommend MT measure the success of their model?

_Operationalizing machine learning_

1. How can MT release their model for production use and avoid their concerns about extremely long delays operationalizing the model? Be specific on how your model is packaged, hosted, and invoked.

2. MT has shown interest in not only scoring a flight at a time (based on a customer's request), but also doing scoring in large chunks so that they could show summaries of predicted flight delays across the United States. What changes would you need to make to your ML model to support this?

_Visualization and reporting_

1. Is Power BI an option for MT to use in visualizing the flight delays?

2. If so, explain:

   a. How would MT load the data and plot it on a map? What specific components would you use and how would you configure them to display the data?

   b. If they need to make minor changes, such as a change to the data types of a column in the model, how would they perform this in Power BI?

   c. How could they secure access to these reports to only their internal customer service agents?

**Prepare**

Directions: With all participants at your table:

1. Identify any customer needs that are not addressed with the proposed solution.

2. Identify the benefits of your solution.

3. Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions

1. Pair with another table.

2. One table is the Microsoft team and the other table is the customer.

3. The Microsoft team presents their proposed solution to the customer.

4. The customer makes one of the objections from the list of objections.

5. The Microsoft team responds to the objection.

6. The customer team gives feedback to the Microsoft team.

7. Tables switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

- Tables reconvene with the larger group to hear a SME share the preferred solution for the case study.

## Additional references

|                                 |                                                                                                    |
| ------------------------------- | :------------------------------------------------------------------------------------------------: |
| **Description**                 |                                             **Links**                                              |
| Azure solution architectures    |                    <https://azure.microsoft.com/en-us/solutions/architecture/>                     |
| Azure Machine Learning services |                 <https://docs.microsoft.com/en-us/azure/machine-learning/service/>                 |
| Machine Learning algorithms     | <https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-cheat-sheet/> |
| Azure Data Factory              |                   <https://docs.microsoft.com/azure/data-factory/introduction/>                    |
| Azure Databricks                |                    <https://docs.microsoft.com/en-us/azure/azure-databricks//>                     |
| Power BI                        |       <https://support.powerbi.com/knowledgebase/articles/430814-get-started-with-power-bi/>       |
| Travel data                     |                           <https://www.transtats.bts.gov/homepage.asp/>                            |
| Weather data                    |                                  <https://darksky.net/dev/docs/>                                   |
| ARM Templates                   |   <https://docs.microsoft.com/azure/azure-resource-manager/resource-group-authoring-templates/>    |
| Azure AD Conditional Access     |              <https://docs.microsoft.com/azure/active-directory/conditional-access/>               |

# Big data and visualization whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your table participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let the table participants begin.

## Step 2: Design a proof of concept solution

- Check in with your tables to ensure that they are transitioning from step to step on time.

- Provide some feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which table will be paired with your table before Step 3 begins.

- For the first round, assign one table as the Microsoft team and the other table as the customer.

- Have the Microsoft team present their solution to the customer team.

  - Have the customer team provide one objection for the Microsoft team to respond to.

  - The presentation and objections and feedback should be no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

Directions: Tables reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Preferred target audience

Jack Tradewinds, CIO of Margie's Travel (MT)

The primary audience is the business decision makers and technology decision makers. From the case study scenario, this would include the Director of Analytics. Usually we talk to the infrastructure managers who report to the chief information officers (CIOs), or to application sponsors (like a vice president \[VP\] line of business \[LOB\], or chief marketing officer \[CMO\]), or to those that represent the business unit IT or developers that report to application sponsors.

## Preferred solution

_High-level architecture_

1. Without getting into the details (the following sections will address the details), diagram your initial vision for handling the top-level requirements for data loading, data preparation, storage, machine learning modeling, and reporting. You will refine this diagram as you proceed.

   After speaking with its supportive team at Microsoft, MT decided that Azure would in fact be the right choice for their platform. They decided to load data into blob storage; explore and prepare it using Spark SQL on Azure Databricks; train a model within Azure Databricks; export the model, save it to Azure Machine Learning service model registry and deploy it as a containerized web service in Azure Kubernetes Service; and visualize the result using a map visualization in Power BI.

   ![This is the high-level overview diagram of the end-to-end solution.](media/high-level-overview.png 'High-level overview diagram')

_NOTE: The preferred solution is only one of many possible, viable approaches._

_Data loading_

1. How would you recommend that MT get their historical data into Azure? What services would you suggest and what are the specific steps they would need to take to prepare the data, to transfer the data, and where would the loaded data land?

   MT should consider using Azure Data Factory (ADF) for copying their historical data into Azure. By setting up a continuous pipeline containing a copy activity configured to copy time partitioned source data, they could pull all their historical information, as well as ingest any future data, into Azure blob storage through a scheduled, and continuously running pipeline. Because their historical data is stored on-premises, MT would need to install and configure an Azure Data Factory Integration Runtime (formerly known as a Data Management Gateway). Once in place, this would allow ADF to copy data from their local data store to a container in blob storage. Their pipeline would be configured to run monthly, as that is the frequency at which new data is received, and this would still allow for all their historical data to be copied without delay.

2. Update your diagram with the data loading process with the steps you identified

   ![The Data loading process diagram begins with Flight Delay Data and Historical Airport Weather Data flat files. An arrow points from there to ADF Copy Pipeline, which in turn points to Blob Storage.](media/data-loading.png 'Data loading process')

_Data preparation_

1. What service would you recommend MT capitalize on to explore the flat files they get from the United States Department of Transportation (USDOT) using SQL?

   Because of their interest in exploring and preparing the flat files using SQL, the best choice here is to use Azure Databricks. It is Microsoft's premier Apache Spark hosted solution that will also be the platform for building and training the machine learning model for flight delay predictions, based on the same data that it was used to prepare. This offers a single platform for data preparation and machine learning, within a collaborative environment that appeals to both data scientists and data engineers.

2. What specific configuration would you use? What components of Azure Databricks would you use to allow MT analysts to query and prep the data? How would they author and execute these data prep tasks?

   They should use Spark SQL within Databricks notebooks. This gives them a simple, interactive interface that allows them to use programming languages such as Python, Scala, and R to prepare their data and train their data models. When a notebook is created or an existing one is opened, they would need to attach it to a cluster. This provides them with a kernel that gives them a preset sqlContext that can be used to run Hive queries using Spark SQL syntax on the data, which allows them to leverage their existing SQL skills. They would need to create the appropriate external tables atop the flight delay files available in HDFS, which reads from the Azure Storage account attached to their Azure Databricks instance.

3. How would you suggest MT integrate weather forecast data?

   They could retrieve the weather forecast data from a third-party service that provides a REST API. An example of such a service is [darksky.net](https://darksky.net).

   ![The Historical Data Preparation flowchart begins with both Flight Delay Data flat files, and Historical Airport Weather Data flat files. Arrows point from the flat files to Blob Storage.](media/historical-data-preparation.png 'Historical Data Preparation flowchart')

_Machine learning modeling_

1. What technology would you recommend that MT use for implementing their machine learning model?

   The model will first be built and trained within an Azure Databricks notebook. The data scientists can use the programming language of their choice (Python, Scala, R, etc.) as well as Spark SQL to featurize and fit the data into the chosen machine learning algorithm. Machine learning libraries such as Spark MLlib or SciKit-Learn can be used within the notebook to simplify things. Once the model is trained and tested with a sufficient amount of historical data, then the model can be exported for deployment to a web service. The model can also continue to be used within Azure Databricks for batch scoring.

2. How would you guide MT to load data, so it can be processed by the machine learning model?

   The data used for training could either be uploaded directly to Azure Databricks, creating a new table with the data stored in DBFS (Databricks File System). Then the data can be accessed from this global persistent table using Spark SQL syntax or DataFrames. Alternately, if they need to train the model with a very large amount of data, the data can be stored in Azure Storage, then the Storage account can be mounted to an Azure Databricks cluster. From that point, the data can be accessed using the wasb/wasbs path and loaded into a DataFrame. Alternately, the data can be persisted into a global table so that it can be easily accessed using Spark SQL syntax and across cluster sessions.

3. What category of machine learning algorithm would you recommend to MT for use in constructing their model? For this scenario, your options are clustering, regression or two-class classification. Why?

   Given that MT only wants a binary prediction of flight delayed or flight not delayed, they should proceed with a two-class classification algorithm, such as logistic regression.

4. Assuming you selected an algorithm that requires training, address the following model design questions:

   a. What is the high-level flow of your machine learning model?

   The data contains a number of useful features (columns) that can be used for scoring. The numeric features may need to be normalized if there are values that are much higher than the others, like sea level pressure (averages around 30) compared to hourly precipitation (usually less than 1, mostly 0) and wind speed (rarely exceeds 10). This can give undo preference to the numeric features containing higher values overall. Next, the categorical columns (text-based), such as origin and destination airports, need to be encoded into numeric indices. They are important features for predicting flight delays, but regression models do not know how to deal with text. You can use a process such as one-hot encoding.

   Once the features have been prepared, you will need to split the data so that the bulk of it (such as 70%) is used to train the model, and the rest of it is used to test, or validate, the model.

   b. What attributes of the flight and weather data do you think MT should use in predicting flight delays? How would you recommend that MT identify the columns that provide the most predictive value in determining if a flight will be delayed? Be specific on the modules or libraries they could use and how they would apply them against the data.

   There are multiple approaches MT could use to perform feature selection and to identify the data attributes that are the most helpful in accurately predicting a delay. They should start with any domain knowledge they have---this would likely point in the direction of the flight attribute's airline, departure airport, destination airport, and time of day as well as the weather attribute's wind speed, temperature, and precipitation conditions. Additionally, they should identify and remove fields that do not add value (i.e., because they are mostly empty or only have a constant value). They should use Python or R functions, such as `replace` or `na.omit`, respectively, to remove empty rows or replace constant values (such as 'M' for missing data) within the important feature columns. From there, they could construct a preliminary model and validate how it performs against the training data. In additional passes, they might choose to score the effectiveness of their trained model against training and testing data sets to see whether their baseline score is improving or regressing.

   c. Some of the data may need a little touching up: columns need to be removed, data types need to be changed. How would these steps be applied in your model?

   Data munging can be best accomplished using R or Python, languages familiar to data scientists and developers. These languages provide powerful data transformation capabilities, and allow for flexibility in how data cleanup occurs, while reducing the overall complexity of ML models. These steps should be performed before featurization. You can use a Spark Pipeline to organize your data transformation steps in order.

   d. How would you recommend MT measure the success of their model?

   For a classification model, they should measure the success of their model against a data set whereby a large portion of the data is used for training purposes, but a smaller set is reserved as examples that will be used to "test" the model to see how it performs against a known outcome. The output of the prediction as well as the "correct" output is passed to the one or more binary evaluation metrics which provide scores such as those from the Confusion Matrix, part of the MLlib MulticlassMetrics library, that give a perspective on accuracy and in what situations the models make mistakes.

   ![The ML Model Creation flowchart begins with prepared data stored in Blob storage. An Azure Databricks notebook is used to create and train the machine learning model from historical data. The trained model is exported and stored within Azure Machine Learning service. Finally, the model is deployed using Azure ML service to a container hosted in Azure Kubernetes Service.](media/ml-model-creation.png 'ML Model Creation flowchart')

_Operationalizing machine learning_

1. How can MT release their model for production use and avoid their concerns about extremely long delays operationalizing the model? Be specific on how your model is packaged, hosted, and invoked.

   Once they have trained their model, it can be exported to any number of common file formats from within Azure Databricks. Because the model is already saved to DBFS and available within a notebook, the easiest way to deploy the model is to use Azure Machine Learning service and the Azure Machine Learning SDK to register the model in Azure ML's model registry and automatically deploy the model to an Azure Kubernetes Service (AKS) cluster. This creates a web service that can be invoked by any REST client, and the cluster can scale to meet demand as needed. This deployed web service takes the weather conditions and flight information as input and returns a response with the classification.

   ![In the On-Demand Delay Predictions flowchart, a Flight Delays Web Portal icon has one arrow labeled "1. Query for weather forecast," pointing to a 3rd Party API, labeled "Forecasted Airport Weather Data (API). A second arrow labeled "2. Query for delay prediction providing weather forecast and flight data," points to containerized AI Services.](media/on-demand-delay-predictions.png 'On-Demand Delay Predictions flowchart')

2. MT has shown interest in not only scoring a flight at a time (based on a customer's request), but also doing scoring in large chunks so that they could show summaries of predicted flight delays across the United States. What changes would you need to make to your ML model to support this?

   The model should be created in such a way that data is prepared and featurized in a consistent way each time it is executed. This way, it can be used equally well for both interactive and batch scoring. The model should be provided within an Azure Databricks notebook that accepts input parameters that point to the file location of the data that needs to be batch processed. This can be implemented via Azure Data Factory (v2) by adding a Linked Service to Azure Databricks and configuring the path and parameters to send to the notebook. By using ADF, they can apply the operational model to data as it is moved to the proper location in Azure storage. The scored results will be available in Blob storage after processing, and with the scheduled pipeline new data will be processed automatically, on the schedule indicated by the pipeline.

   The notebook that is executed also writes the summarized flight delay predictions to an Azure SQL Database. This data will be used by Power BI to display the predictions in a series of reports and dashboards.

_Visualization and reporting_

1. Is Power BI an option for MT to use in visualizing the flight delays?

   Yes, Power BI is a good option for MT. Power BI can perform what is called a Direct Query against Apache Spark hive data sources as well as an Import query that copies the data into Power BI managed datasets from Spark. However, connecting Power BI to an Azure Databricks cluster requires it to be running any time a report is displayed. A more cost-effective solution is to write the summarized data to an Azure SQL Database each time the notebook is executed during the batch scoring process. In this case, Power BI uses Direct Query against the Azure SQL Database instead, allowing MT to terminate clusters that are not in use.

2. If so, explain:

   a. How would MT load the data and plot it on a map? What specific components would you use and how would you configure them to display the data?

   The data would be set up as a query against Azure SQL Database that loads the predicted flight delay for each airport location. They would use the map visualization to display a delay indicator for a particular flight at a particular airport.

   ![A map displays with two large dots over Los Angeles and San Diego.](media/visualization-map.png 'visualization map')

   b. If they need to make minor changes, such as a change to the data types of a column in the model, how would they perform this in Power BI?

   They could do this using the Query Editor component of the Power BI Desktop application. They could then upload this file to the Power BI service.

   c. How could they secure access to these reports to only their internal customer service agents?

   By utilizing the Power BI service, they can create a Content Pack that contains only the desired Dashboards, Reports, and Datasets and restrict access to those Groups in Azure Active Directory to which the customer service agents belong.

   ![In the Visualizing Bulk Delay Predictions diagram, Flight Delays Web Portal has an arrow labeled "1. Load Power BI embedded report," pointing to a Power BI icon labeled Visualize Delay Predictions on a Map. An arrow labeled "2. Report uses Power BI Direct Query against source and caches it," points from the Power BI icon to an Azure SQL Database icon.](media/visualizing-bulk-delay-predictions.png 'Visualizing Bulk Delay Predictions diagram')

## Checklist of preferred objection handling

1. We have heard that creating a machine learning model takes a month to build and another 2-3 months to operationalize so that it is useable from our production systems. Is this true?

   This is true in the traditional process for creating machine learning models, whereby the data scientist creates a model (e.g., in R) and then hands it over to developers who translate it into Java or C\#---which can take months to get the translation correct and performant. With Azure Machine Learning service, you can deploy Docker-based container images with a single command to Azure Kubernetes Service managed by the Machine Learning service workspace. During the process, a REST API for the deployed model is generated along with a swagger document. This makes it very easy for clients to consume the model.

2. Once our model is operationalized, how do we retrain and redeploy it? Will this process break clients currently accessing the deployed model?

   Azure Machine Learning service provides APIs that you can use to retrain your models. You can also use the APIs to update existing deployments with updated versions of the model. As part of the data science workflow, you recreate the model in your experimentation environment. Then, you register the model with Azure Machine Learning service model registry and update existing deployments. Updates are performed using a single UPDATE CLI command. The UPDATE command updates existing deployments without changing the API URL or the key. The applications consuming the model continue to work without any code change, and start getting better predictions using new model.

3. Can we query flat files in the file system using SQL?

   Yes. There are many options for using a SQL syntax to query files in Blob storage such as Azure Databricks, SQL Data Warehouse, and HDInsight with Spark SQL or Hive.

4. Does Azure provide anything that would speed up querying (and exploration) of files in Hadoop Distributed File Systems (HDFS)?

   Yes. Hive on HDInsight provides the ability to create indices on flat file content, as does SQL Data Warehouse. Azure Databricks provides a distributed in-memory cache that can aid exploratory queries that often repeat queries against the same subsets of data.

5. Does Azure provide any tools for visualizing our data? Ideally access to these could be managed with Active Directory.

   Power BI is available as a service and provides tools for creating both dashboards and reports whose access can be restricted by group membership in Azure Active Directory.

6. Can we use Azure Active Directory accounts for our users, and if so, can we restrict who can access Azure Databricks, when they can access it, require two-factor authentication, and restrict access if there is suspicious activity on their account?

   Azure Databricks is automatically integrated with Azure Active Directory (Azure AD). This allows you to control access to your workspace and allows users to sign in with their Microsoft or organizational account. Since users can sign in from a variety of devices from anywhere, simply focusing on who can access a resource is not sufficient anymore. Azure AD conditional access helps address this requirement by implementing automated access control decisions based on conditions. With conditional access, you can restrict sign-ins to your corporate network, require multi-factor authentication, restrict hours in which users can log in, and restrict access if there is a detected sign-in risk.

   To enable conditional access for Azure Databricks, perform the following steps:

   1. In the Azure Portal, open the **Azure Active Directory** service.

   2. Select **Conditional access** in the SECURITY section.

   3. Select **New policy** to create a new conditional access policy.

   4. In **Cloud apps**, click **Select apps**, and then search for the application ID `2ff814a6-3304-4ab8-85cb-cd0e6f879c1d`. Select **AzureDatabricks**.

      ![Selecting AzureDatabricks under Cloud Apps.](media/azure-databricks-conditional-access.png 'Cloud Apps')

   5. Fill in the other settings according to your desired conditional access configuration.

7. Is Azure Databricks our only option for running SQL on Hadoop solutions in Azure?

   No. Azure HDInsight provides an option for running hosted Spark clusters as well. Within HDInsight, you can provision hosted clusters based on Hive, HBase, and others. Hortonworks, Cloudera, and MapR provide fully supported distributions that are available in the Azure Marketplace and provide environments for running HiveQL Spark SQL in Azure.

8. We have heard of Azure Data Lake, but we are not clear about whether this is currently a good fit for our PoC solution, or whether we should be using it for interactive analysis of our data

   There are two offerings related to Azure Data Lake that should be called out separately. Azure Data Lake Analytics provides a distributed querying engine whose costs are based solely on the number of resources used in the query, the amount of time those resources were made available to the query, and a nominal job completion charge. In its current form, Azure Data Lake Analytics supports only Batch workloads, so it is not the right tool for interactive analytic workloads.

   Azure Data Lake Storage Gen2 provides the ability to store an unlimited number of items, each of unlimited size, having no upper limit on the total storage capacity. This makes it more capable than Azure Blob Storage since it removes the 2 PB account size limit, the page blob size limit, and the block blob limit imposed by Blob storage. However, given MT's low data growth projection, this is not a concern for them.

   Another differentiator between the two storage services is that Blob Storage uses a flat storage schema with path names as part of the bob file names. This makes the WASB driver Spark uses to connect to Blob Storage for HDFS access do extra work to map file structures to blob paths. This can lead to degraded performance in cases of high volume and many small files. In contrast, ADLS Gen2 has hierarchical storage that provides granular POSIX permissions and Access Control Lists (ACLs). This provides greater flexibility in controlling access to files, and Spark uses the newer ABFS driver which can provide higher performance. However, MT's requirements do not include fine-grained permissions and hierarchical storage, and the size and velocity of their data will mean that they would likely never notice a significant performance difference between the two options.

   Given their requirements and comparison of services, Blob Storage is recommended as it meets MT's requirements, and is a more cost-effective solution.

## Customer quote (to be read back to the attendees at the end)

_"We are flying into the future with Azure, helping our customers more aggressively schedule their travel and optimize their non-travel time."_

Jack Tradewinds, CIO of Margie's Travel
